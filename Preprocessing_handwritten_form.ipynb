{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing a handwritten dataset from stamps and block letters.\n",
    "\n",
    "Copyright (c) 2020 NORLIST.kz\n",
    "Written by Galymzhan Abdimanap.\n",
    "Version 1.0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Import library.\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Import Tesseract.\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# Import Mask RCNN\n",
    "# To find local version of the library\n",
    "# For the independence of the program from the built-in OS libraries, the Mask RCNN library is included in the program folder\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "box_ind is deprecated, use box_indices instead\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n",
      "\n",
      "WARNING:tensorflow:From /media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "/media/manapov/7ff2a4aa-26aa-48f8-ae2a-9fba42bc53431/Work/POST_project/Stamp_text_drop/weights/seal20200221T1748/mask_rcnn_seal_0030.h5\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/manapov/miniconda3/envs/seal_detection/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "Re-starting from epoch 30\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Mask RCNN model\n",
    "#------------------------------------------------------------------------------\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\"\")\n",
    "\n",
    "sys.path.append(ROOT_DIR)\n",
    "WEIGHTS_DIR_NAME = \"weights\"\n",
    "\n",
    "# Ð¡hoose device type\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\n",
    "\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, WEIGHTS_DIR_NAME)\n",
    "\n",
    "class SealConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy  dataset.\n",
    "    Derives from the base Config class and overrides some values.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"seal\"\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\n",
    "    # Adjust down if you use a smaller GPU.\n",
    "    IMAGES_PER_GPU = 1\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + seal\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.95\n",
    "\n",
    "config = SealConfig()\n",
    "\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Run detection on one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model in inference mode\n",
    "with tf.device(DEVICE):\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Or, load the last model you trained\n",
    "weights_path = model.find_last()\n",
    "#weights_path = os.path.abspath('G:\\Stuff\\Daniyar\\Development\\Python\\detectStamp\\logs\\seal20200221T1748\\mask_rcnn_seal_0030.h5')\n",
    "print(weights_path)\n",
    "# Load weights\n",
    "model.load_weights(weights_path, by_name = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Adjust the size attribute to control how big to render images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# Stamp detection\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def detect_seal(image, min_probability = 0.91, min_width_height_ratio = 0.85):\n",
    "    \"\"\"Detects stamp (seal) in given image.\n",
    "    Returns positions of stamp.\n",
    "    \"\"\"\n",
    "    assert image is not None\n",
    "\n",
    "    results_rois = []\n",
    "    results_scores = []\n",
    "\n",
    "   \n",
    "    # Detect the stamp.\n",
    "    results = model.detect([image], graph)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Parse detected result\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    r = results[0] \n",
    "    rois = r['rois'] # format r['rois']=[y1,x1,y2,x2]\n",
    "    scores = r['scores']\n",
    "    masks = r['masks']\n",
    "    \n",
    "    \n",
    "    # For visualize process detects stamp.\n",
    "    # ax = get_ax(1)\n",
    "    # visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "    #                         'seal', r['scores'], ax=ax,\n",
    "    #                         title=\"Predictions\")\n",
    "    \n",
    "    # print(r['rois'])\n",
    "    return r['rois']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_img(filename, img, rois):\n",
    "    '''Coloring the circle with white. Drop the stamp.'''\n",
    "    for i, el in enumerate(rois): \n",
    "        center =(int(rois[i][1] + (rois[i][3] - rois[i][1])/2), int(rois[i][0] + (rois[i][2] - rois[i][0])/2)) # (x,y)\n",
    "        radius = int((rois[i][2] - rois[i][0])/2)\n",
    "        color = (255, 255, 255)\n",
    "        cv2.circle(img, center, radius + 30, color, -1)\n",
    "    return img\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_positions_form_kaznitu(image):\n",
    "    \"\"\"Get positions of area for crop form KAZNITU. Return y1, y2, h.\n",
    "    We get the positions of two specific words and\n",
    "    color the area between these words with white.\"\"\"\n",
    "    \n",
    "    # Load the input image, convert it from BGR to RGB channel ordering,\n",
    "    # and use Tesseract to localize each area of text in the input image.\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pytesseract.image_to_data(rgb, output_type=Output.DICT, lang='kaz')\n",
    "\n",
    "    FIRST_KEY_WORD = \"ÐÐÐÐÐ¡Ð¢ÐÐ Ð¡Ð¢ÐÐ\"\n",
    "    LAST_KEY_WORD = \"ÐÐÐ\"\n",
    "    FIRST_KEY_WORD_FOOTER = \"ÐÐ°Ð·ÐÐÐ¢Ð£\"\n",
    "    y_first_key_word_footer = None\n",
    "    y_first_key_word = None\n",
    "    y_last_key_word = None\n",
    "    h_last_key_word = None\n",
    "\n",
    "\n",
    "    # loop over each of the individual text localizations\n",
    "    for i in range(0, len(results[\"text\"])):\n",
    "        if results[\"text\"][i] == FIRST_KEY_WORD:\n",
    "            # extract the bounding box coordinates of the text region from\n",
    "            # the current result\n",
    "            x_first_key_word = results[\"left\"][i]\n",
    "            y_first_key_word = results[\"top\"][i]\n",
    "            w_first_key_word = results[\"width\"][i]\n",
    "            h_first_key_word = results[\"height\"][i]\n",
    "\n",
    "        if results[\"text\"][i] == LAST_KEY_WORD:\n",
    "            # extract the bounding box coordinates of the text region from\n",
    "            # the current result\n",
    "            x_last_key_word = results[\"left\"][i]\n",
    "            y_last_key_word = results[\"top\"][i]\n",
    "            w_last_key_word = results[\"width\"][i]\n",
    "            h_last_key_word = results[\"height\"][i]\n",
    "\n",
    "        if results[\"text\"][i] == FIRST_KEY_WORD_FOOTER:\n",
    "            # extract the bounding box coordinates of the text region from\n",
    "            # the current result\n",
    "            x_first_key_word_footer = results[\"left\"][i]\n",
    "            y_first_key_word_footer = results[\"top\"][i]\n",
    "            w_first_key_word_footer = results[\"width\"][i]\n",
    "            h_first_key_word_footer = results[\"height\"][i]\n",
    "        \n",
    "        if y_first_key_word_footer is not None:     \n",
    "            cv2.rectangle(image, (0, y_first_key_word_footer), (image.shape[1], image.shape[0]), (255, 255, 255), -1)\n",
    "    \n",
    "    return y_first_key_word, y_last_key_word, h_last_key_word\n",
    "\n",
    "    \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_positions_form_kaznu(image):\n",
    "    \"\"\"Get positions of area for crop form KAZNU. Return y1, y2, h.\n",
    "    We get the positions of two specific words and\n",
    "    color the area between these words with white.\"\"\"\n",
    "    \n",
    "    # Load the input image, convert it from BGR to RGB channel ordering,\n",
    "    # and use Tesseract to localize each area of text in the input image.\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pytesseract.image_to_data(rgb, output_type=Output.DICT, lang='kaz')\n",
    "\n",
    "    FIRST_KEY_WORDS = [\"ÓÐ»-ÑÐ°ÑÐ°Ð±Ð¸\", \"Ð¿ÐµÑÐ°ÑÑ\", \"Ð±Ð¸Ð»ÐµÑÐ¾Ð²\"]\n",
    "    LAST_KEY_WORDS = [\"Ð¼.Ð¾.\", \"Ð¼Ð°Ð½ÑÑÑÐ¾Ð²Ð°\", \"Ð¼ÑÑÐ¸ÑÐ°Ð»Ð¸ÐµÐ²Ð°\", \"Ð½Ò±ÑÐ¼Ò±ÑÐ°Ð½\", \"ÑÑÑÐ»ÑÐ±ÐµÐºÐ¾Ð²Ð°\", \"ÑÐ°Ð±Ð°ÐµÐ²\", \"Ð°ÒÐ¶ÑÐ³ÑÑÐ¾Ð²Ð°\"]\n",
    "    FLAG_ON_FIRST_WORD = 0\n",
    "    FLAG_ON_LAST_WORD = 0\n",
    "    \n",
    "    y_first_key_word = None\n",
    "    y_last_key_word = None\n",
    "    h_last_key_word = None\n",
    "\n",
    "\n",
    "    # loop over each of the individual text localizations\n",
    "    for i in range(0, len(results[\"text\"])):\n",
    "        if results[\"text\"][i].lower() in FIRST_KEY_WORDS and FLAG_ON_FIRST_WORD == 0:\n",
    "            # extract the bounding box coordinates of the text region from\n",
    "            # the current result\n",
    "            x_first_key_word = results[\"left\"][i]\n",
    "            y_first_key_word = results[\"top\"][i]\n",
    "            w_first_key_word = results[\"width\"][i]\n",
    "            h_first_key_word = results[\"height\"][i]\n",
    "            FLAG_ON_FIRST_WORD = 1\n",
    "\n",
    "        if results[\"text\"][i].lower() in LAST_KEY_WORDS and FLAG_ON_LAST_WORD == 0:\n",
    "            # extract the bounding box coordinates of the text region from\n",
    "            # the current result\n",
    "            x_last_key_word = results[\"left\"][i]\n",
    "            y_last_key_word = results[\"top\"][i]\n",
    "            w_last_key_word = results[\"width\"][i]\n",
    "            h_last_key_word = results[\"height\"][i]\n",
    "            FLAG_ON_LAST_WORD = 1\n",
    "    \n",
    "    return y_first_key_word, y_last_key_word, h_last_key_word\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Script run. \n",
    "\n",
    "\n",
    "IMAGE_FILES_PATH = 'dataset/20200921_Examination_sheets/*.jpg'\n",
    "CLEANED_IMAGE_FILES_PATH = 'dataset/cleaned_images_20200921_Examination_sheets/'\n",
    "HANDWRITTEN_FORM = 'KAZNU' # 'KAZNITU'  or  'KAZNU'\n",
    "\n",
    "\n",
    "# Get a list with images filename.\n",
    "images_filename = glob.glob(IMAGE_FILES_PATH)\n",
    "\n",
    "counter = 0\n",
    "for image_filename in images_filename:\n",
    "    # img = cv2.imread(image_filename)\n",
    "    # For read images with cirilyc letters.\n",
    "    img = cv2.imdecode(np.fromfile(image_filename, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "    # Detects stamp (seal) in given image.\n",
    "    rois = detect_seal(img)\n",
    "    # If find a stamp on the image.\n",
    "    if len(rois)>0:\n",
    "        img = cleaning_img(os.path.basename(image_filename), img, rois)\n",
    "        \n",
    "    \n",
    "    if HANDWRITTEN_FORM == 'KAZNITU':\n",
    "#         id_form = 0\n",
    "        # Get positions of area for crop form KAZNITU.\n",
    "        y_first_key_word, y_last_key_word, h_last_key_word = get_crop_positions_form_kaznitu(img)\n",
    "    elif HANDWRITTEN_FORM == 'KAZNU':\n",
    "#         id_form = 1\n",
    "        # Get positions of area for crop form KAZNU.\n",
    "        y_first_key_word, y_last_key_word, h_last_key_word = get_crop_positions_form_kaznu(img)\n",
    "    else:\n",
    "        print(\"Please choose HANDWRITTEN_FORM! KAZNITU or KAZNU\")\n",
    "        break\n",
    "    \n",
    "    # print(image_filename)\n",
    "    # Coloring the area with white. Drop the words.\n",
    "    if y_last_key_word and h_last_key_word is not None:\n",
    "        cv2.rectangle(img, (0, 0), (img.shape[1], y_last_key_word + h_last_key_word + 50), (255, 255, 255), -1)\n",
    "    elif  y_first_key_word or y_last_key_word or h_last_key_word is not None:\n",
    "        print(\"Error image: \", image_filename)\n",
    "        print(\"Error image: \", str(counter) + '.jpg' )\n",
    "    \n",
    "    #  Save cleaned image.\n",
    "    cv2.imwrite(CLEANED_IMAGE_FILES_PATH + str(counter) + '.jpg', img)\n",
    "    counter += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = cv2.imread('images/1_7.jpg')\n",
    "\n",
    "# # Adding custom options\n",
    "# custom_config = r'--oem 3 --psm 6'\n",
    "# pytesseract.image_to_data(img, output_type=Output.DICT, lang='kaz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
